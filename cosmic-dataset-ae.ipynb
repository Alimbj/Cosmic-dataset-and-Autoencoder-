{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"mount_file_id":"1-j_fmGO_r86I9cIFFvQojAEuBN0cj5_7","authorship_tag":"ABX9TyOWySs6pd8HxNYhr9nw1QSz"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10060638,"sourceType":"datasetVersion","datasetId":6199792},{"sourceId":10078141,"sourceType":"datasetVersion","datasetId":6212634},{"sourceId":10092513,"sourceType":"datasetVersion","datasetId":6223561},{"sourceId":10101640,"sourceType":"datasetVersion","datasetId":6230593},{"sourceId":10170322,"sourceType":"datasetVersion","datasetId":6281073},{"sourceId":10931963,"sourceType":"datasetVersion","datasetId":6797558}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pysam\n\n!pip install pyfaidx\n\n!pip install bionumpy\n\n!pip install biopython","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyvXbd_gp3_K","executionInfo":{"status":"ok","timestamp":1732980951371,"user_tz":480,"elapsed":15215,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"outputId":"24f63138-abe7-4596-c103-822c8aa6200b","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:43:26.412193Z","iopub.execute_input":"2025-03-22T10:43:26.412558Z","iopub.status.idle":"2025-03-22T10:43:50.862953Z","shell.execute_reply.started":"2025-03-22T10:43:26.412527Z","shell.execute_reply":"2025-03-22T10:43:50.861739Z"}},"outputs":[{"name":"stdout","text":"Collecting pysam\n  Downloading pysam-0.23.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nDownloading pysam-0.23.0-cp310-cp310-manylinux_2_28_x86_64.whl (22.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: pysam\nSuccessfully installed pysam-0.23.0\nCollecting pyfaidx\n  Downloading pyfaidx-0.8.1.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from pyfaidx) (8.5.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pyfaidx) (24.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->pyfaidx) (3.20.2)\nDownloading pyfaidx-0.8.1.3-py3-none-any.whl (28 kB)\nInstalling collected packages: pyfaidx\nSuccessfully installed pyfaidx-0.8.1.3\nCollecting bionumpy\n  Downloading bionumpy-1.0.13-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from bionumpy) (1.26.4)\nCollecting npstructures>=0.2.15 (from bionumpy)\n  Downloading npstructures-0.2.19-py2.py3-none-any.whl.metadata (5.2 kB)\nDownloading bionumpy-1.0.13-py2.py3-none-any.whl (165 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading npstructures-0.2.19-py2.py3-none-any.whl (36 kB)\nInstalling collected packages: npstructures, bionumpy\nSuccessfully installed bionumpy-1.0.13 npstructures-0.2.19\nCollecting biopython\n  Downloading biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\nDownloading biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install scikit-fuzzy\n# pip install kaleido","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:00.322212Z","iopub.execute_input":"2025-03-10T16:25:00.322532Z","iopub.status.idle":"2025-03-10T16:25:00.337850Z","shell.execute_reply.started":"2025-03-10T16:25:00.322499Z","shell.execute_reply":"2025-03-10T16:25:00.336961Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"# pip install dask","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B4leX0x1a3jF","executionInfo":{"status":"ok","timestamp":1732980965213,"user_tz":480,"elapsed":8689,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"outputId":"1917c355-f5ee-4d58-fa83-dd4affaa76db","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:00.339056Z","iopub.execute_input":"2025-03-10T16:25:00.339439Z","iopub.status.idle":"2025-03-10T16:25:00.355164Z","shell.execute_reply.started":"2025-03-10T16:25:00.339403Z","shell.execute_reply":"2025-03-10T16:25:00.354336Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"!pip install hdbscan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:43:50.864450Z","iopub.execute_input":"2025-03-22T10:43:50.864820Z","iopub.status.idle":"2025-03-22T10:43:55.706430Z","shell.execute_reply.started":"2025-03-22T10:43:50.864784Z","shell.execute_reply":"2025-03-22T10:43:55.704981Z"}},"outputs":[{"name":"stdout","text":"Collecting hdbscan\n  Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.26.4)\nRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.2.2)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->hdbscan) (3.5.0)\nDownloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: hdbscan\nSuccessfully installed hdbscan-0.8.40\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pysam\nfrom Bio import SeqIO\nimport tarfile\nimport pandas as pd\nimport numpy as np\n# import bionumpy as bnp\n# import dask.dataframe as dd\n# from bionumpy.io.delimited_buffers import DelimitedBuffer\n# from bionumpy.bnpdataclass import bnpdataclass\nfrom pyfaidx import Fasta\nimport shutil\nimport gzip\n\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n# import torch_xla\n# import torch_xla.core.xla_model as xm\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom sklearn.cluster import DBSCAN\nimport gc\nfrom tqdm import tqdm\nimport time\nfrom itertools import product\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nimport hdbscan\n#import skfuzzy\n#from skfuzzy import cmeans\nfrom sklearn.preprocessing import StandardScaler\n#from mpl_toolkits.mplot3d import Axes3D\n#import plotly.graph_objects as go\nfrom torch.cuda.amp import GradScaler, autocast","metadata":{"id":"5yD3jPZbqFr4","executionInfo":{"status":"ok","timestamp":1732980972090,"user_tz":480,"elapsed":4796,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6549573a-0fe7-47ae-fe6d-88b8afb56dd3","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:43:55.708544Z","iopub.execute_input":"2025-03-22T10:43:55.708959Z","iopub.status.idle":"2025-03-22T10:44:02.163550Z","shell.execute_reply.started":"2025-03-22T10:43:55.708915Z","shell.execute_reply":"2025-03-22T10:44:02.162385Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# # Разархивируем файл с мутациями\n# tar_file_path = '/content/drive/MyDrive/Cosmic_GenomeScreensMutant_VcfNormal_v101_GRCh38.tar'\n# with tarfile.open(tar_file_path, 'r') as tar:\n#   tar.extractall('/content/drive/MyDrive')\n#   print(f'Extracted file: {tar.getnames()}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DhRNMvo-qFv5","executionInfo":{"status":"ok","timestamp":1732544386957,"user_tz":480,"elapsed":29359,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"outputId":"51c942b1-07fa-4a84-d36a-71e7cff4e78e","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:06.962662Z","iopub.execute_input":"2025-03-10T16:25:06.962894Z","iopub.status.idle":"2025-03-10T16:25:06.984432Z","shell.execute_reply.started":"2025-03-10T16:25:06.962876Z","shell.execute_reply":"2025-03-10T16:25:06.983806Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# # Парсим файл с мутациями\n# vcf_file_path = '/content/drive/MyDrive/Cosmic_GenomeScreensMutant_Normal_v101_GRCh38.vcf.gz'\n# # row_limit = 20000000\n# # data = []\n# # Open the VCF file\n# with pysam.VariantFile(vcf_file_path, \"r\") as vcf_file:\n#     for row_count, record in enumerate(vcf_file.fetch()):\n#         data.append({\n#             'Chromosome': record.chrom,\n#             'Position': record.pos,\n#             'ID': record.id,\n#             'Reference': record.ref      \n#             })\n#         # if row_count + 1 >= row_limit:\n#         #     break\n# df = pd.DataFrame(data)\n# # Extract INFO fields into separate columns\n# # info_df = df['Info'].apply(pd.Series)\n# # df = pd.concat([df.drop(columns=['Info']), info_df], axis=1)\n# print(df.head())  # Preview the first rows","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVVEnKQ8qFx2","executionInfo":{"status":"ok","timestamp":1732876223653,"user_tz":480,"elapsed":189568,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"outputId":"b07fe45f-f6e0-4d0a-cfca-36e15f851264","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:06.985664Z","iopub.execute_input":"2025-03-10T16:25:06.985952Z","iopub.status.idle":"2025-03-10T16:25:06.999249Z","shell.execute_reply.started":"2025-03-10T16:25:06.985930Z","shell.execute_reply":"2025-03-10T16:25:06.998634Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# Сохраним полученный файл с мутациями\n#df.to_csv('Cosmic_df.csv', index=False)","metadata":{"id":"JYL_I4xjeN96","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T08:47:49.215848Z","iopub.execute_input":"2025-03-22T08:47:49.216252Z","iopub.status.idle":"2025-03-22T08:47:49.221184Z","shell.execute_reply.started":"2025-03-22T08:47:49.216222Z","shell.execute_reply":"2025-03-22T08:47:49.219939Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Разархивируем полногеномный файл человека если формат gzip\n# with gzip.open(' ', 'rt') as file:\n#   with open(\"GCF_000001405.40_GRCh38.p14_genomic.fna\", 'w') as file_out:\n#     shutil.copyfileobj(file, file_out)","metadata":{"id":"IHW9HBUDNtcj","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T08:59:51.443783Z","iopub.execute_input":"2025-03-22T08:59:51.444204Z","iopub.status.idle":"2025-03-22T08:59:51.448292Z","shell.execute_reply.started":"2025-03-22T08:59:51.444171Z","shell.execute_reply":"2025-03-22T08:59:51.446967Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# chunk_size = 5000000\n# chunks = []\n# for chunk in pd.read_csv('/kaggle/input/cosmic-completetargetedscreensmutant/Cosmic_CompleteTargetedScreensMutant_v101_GRCh38.tsv/Cosmic_CompleteTargetedScreensMutant_v101_GRCh38.tsv', \n#                          sep = '\\t',\n#                          usecols=['TRANSCRIPT_ACCESSION', 'COSMIC_SAMPLE_ID', 'COSMIC_PHENOTYPE_ID', \n#                                   'GENOMIC_MUTATION_ID', 'MUTATION_CDS', 'MUTATION_DESCRIPTION', \n#                                   'MUTATION_ZYGOSITY', 'CHROMOSOME', 'STRAND', 'MUTATION_SOMATIC_STATUS', \n#                                   'POSITIVE_SCREEN'],\n#                          chunksize=chunk_size):\n#     chunks.append(chunk)\n# targeted_screen = pd.concat(chunks, ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.428560Z","iopub.status.idle":"2025-03-10T16:25:15.428959Z","shell.execute_reply":"2025-03-10T16:25:15.428776Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targeted_screen.to_csv('targeted_screen_mut.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.429973Z","iopub.status.idle":"2025-03-10T16:25:15.430426Z","shell.execute_reply":"2025-03-10T16:25:15.430228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# nan_percentage = targeted_screen.isna().mean() * 100\n# print(nan_percentage)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.431214Z","iopub.status.idle":"2025-03-10T16:25:15.431596Z","shell.execute_reply":"2025-03-10T16:25:15.431422Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hallmarks = pd.read_csv('/kaggle/input/cosmic-data/Cosmic_CancerGeneCensusHallmarksOfCancer_v101_GRCh38.tsv', sep='\\t')\nclassification = pd.read_csv('/kaggle/input/cosmic-data/Cosmic_Classification_v101_GRCh38.tsv', sep='\\t')\ntranscripts = pd.read_csv('/kaggle/input/cosmic-data/Cosmic_Transcripts_v101_GRCh38.tsv', sep='\\t')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.432498Z","iopub.status.idle":"2025-03-10T16:25:15.432744Z","shell.execute_reply":"2025-03-10T16:25:15.432642Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transcripts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.433532Z","iopub.status.idle":"2025-03-10T16:25:15.433927Z","shell.execute_reply":"2025-03-10T16:25:15.433743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Парсим файл генома человека\nfna_file_path = \"/kaggle/input/gcf-000001405-40-grch38-p14-genomic-fna-gz/GCF_000001405.40_GRCh38.p14_genomic.fna\"\n# with open(fna_file_path, \"r\") as fna_file:\n#     for record in SeqIO.parse(fna_file, \"fasta\"):\n#         print(f\"ID: {record.id}\")\n#         print(f\"Description: {record.description}\")\n#         print(f\"Sequence Length: {len(record.seq)}\")\n#         print(f\"First 100 Bases: {record.seq[:100]}\")\n#         print(\"-\" * 50)","metadata":{"id":"G4BiRRScYOcY","executionInfo":{"status":"ok","timestamp":1732980977324,"user_tz":480,"elapsed":343,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:44:07.463623Z","iopub.execute_input":"2025-03-22T10:44:07.464157Z","iopub.status.idle":"2025-03-22T10:44:07.468691Z","shell.execute_reply.started":"2025-03-22T10:44:07.464126Z","shell.execute_reply":"2025-03-22T10:44:07.467322Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Проверим совпадения позиций в двух датасетах\ntarget_position = 65797\ntarget_chromosome = 'NC_000001.11'\nfor record in SeqIO.parse(fna_file_path, \"fasta\"):\n    if record.id == 'NC_000001.11':  # Проверяем идентификатор хромосомы\n        # Позиции в Biopython индексируются с нуля, поэтому вычитаем 1\n        nucleotide = record.seq[target_position-1]\n        print(f\"Нуклеотид на позиции {target_position} в хромосоме {target_chromosome}: {nucleotide}\")\n        break\nelse:\n    print(f\"Хромосома {target_chromosome} не найдена в файле.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZfxNqtZtflYy","executionInfo":{"status":"ok","timestamp":1732977639144,"user_tz":480,"elapsed":1923,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"outputId":"694047a0-8e51-443f-997b-c2dfcdae3359","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:44:16.191828Z","iopub.execute_input":"2025-03-22T10:44:16.192228Z","iopub.status.idle":"2025-03-22T10:44:21.536407Z","shell.execute_reply.started":"2025-03-22T10:44:16.192198Z","shell.execute_reply":"2025-03-22T10:44:21.535282Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Нуклеотид на позиции 65797 в хромосоме NC_000001.11: T\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Создадим словарь для соответствия названий хромосом и их идентификаторов в полногеномнике\nchromosome_dict = {\n    1: 'NC_000001.11',\n    #2: 'NC_000002.12',\n    3: 'NC_000003.12',\n    4: 'NC_000004.12',\n    5: 'NC_000005.10',\n    6: 'NC_000006.12',\n    7: 'NC_000007.14',\n    8: 'NC_000008.11',\n    9: 'NC_000009.12',\n    10: 'NC_000010.11',\n    11: 'NC_000011.10',\n    12: 'NC_000012.12',\n    13: 'NC_000013.11',\n    14: 'NC_000014.9',\n    15: 'NC_000015.10',\n    16: 'NC_000016.10',\n    17: 'NC_000017.11',\n    18: 'NC_000018.10',\n    19: 'NC_000019.10',\n    20: 'NC_000020.11',\n    21: 'NC_000021.9',\n    22: 'NC_000022.11',\n    'X': 'NC_000023.11',\n    'Y': 'NC_000024.10'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:45:49.489725Z","iopub.execute_input":"2025-03-22T10:45:49.490105Z","iopub.status.idle":"2025-03-22T10:45:49.495637Z","shell.execute_reply.started":"2025-03-22T10:45:49.490075Z","shell.execute_reply":"2025-03-22T10:45:49.494606Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# # Читаем референсный геном и сохраняем его в словарь\ndef load_primary_assembly(fna_file_path):\n    genome = {}\n    for record in SeqIO.parse(fna_file_path, \"fasta\"):\n        if \"Primary Assembly\" in record.description and \"unlocalized\" not in record.description and \"unplaced\" not in record.description:\n            genome[record.id] = record.seq\n    return genome\n\n# Загружаем только primary assembly\nhg38_genome = load_primary_assembly(\"/kaggle/input/gcf-000001405-40-grch38-p14-genomic-fna-gz/GCF_000001405.40_GRCh38.p14_genomic.fna\")\n\n# Загружаем датасет мутаций\n# mutation_dataset = pd.read_csv('/kaggle/input/cosmic-df-csv/Cosmic_df.csv')\n\n# Проверка загруженных данных\nprint(f\"Число загруженных хромосом: {len(hg38_genome)}\")\nfor chrom in list(hg38_genome.keys())[:5]:  # Выведем первые 5 хромосом\n    print(f\"ID: {chrom}, Длина: {len(hg38_genome[chrom])}\")","metadata":{"id":"mxW4hYhNSgJT","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:46:42.367645Z","iopub.execute_input":"2025-03-22T10:46:42.368045Z","iopub.status.idle":"2025-03-22T10:47:39.369938Z","shell.execute_reply.started":"2025-03-22T10:46:42.368016Z","shell.execute_reply":"2025-03-22T10:47:39.368710Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Число загруженных хромосом: 24\nID: NC_000001.11, Длина: 248956422\nID: NC_000002.12, Длина: 242193529\nID: NC_000003.12, Длина: 198295559\nID: NC_000004.12, Длина: 190214555\nID: NC_000005.10, Длина: 181538259\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# def get_flanking_seq(row, genome, flank_size=100):\n#     chrom_id = chromosome_dict.get(row['Chromosome'])\n#     if not chrom_id:\n#         return None  # Если хромосома не найдена в словаре\n#     position = row['Position']\n#     try:\n#         seq = genome[chrom_id]  # Получаем последовательность хромосомы\n#         start = max(0, position - flank_size - 1)  # Начальная позиция (учитываем границы)\n#         end = position + flank_size  # Конечная позиция\n#         flanking_seq = seq[start:end]  # Вырезаем фланкирующую последовательность\n#         return str(flanking_seq)  # Преобразуем в строку\n#     except KeyError:\n#         return None  # Если хромосома отсутствует в референсе\n# # Применяем функцию к каждой строке файла с мутациями\n# mutation_dataset['flanking_seq'] = mutation_dataset.apply(\n#     lambda row: get_flanking_seq(row, hg38_genome), axis=1\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.439696Z","iopub.status.idle":"2025-03-10T16:25:15.440054Z","shell.execute_reply":"2025-03-10T16:25:15.439923Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mutation_dataset['flanking_seq'] = mutation_dataset['flanking_seq'].apply(lambda seq: seq.upper() if pd.notnull(seq) else seq)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"LCccMHj1osW2","executionInfo":{"status":"error","timestamp":1732981216722,"user_tz":480,"elapsed":336,"user":{"displayName":"Alim Bijiev","userId":"17102337830952167924"}},"outputId":"5ed7e05e-183a-462c-c878-70a9df15936c","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.440696Z","iopub.status.idle":"2025-03-10T16:25:15.440951Z","shell.execute_reply":"2025-03-10T16:25:15.440846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mutation_dataset.to_csv('mut_flank.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.441724Z","iopub.status.idle":"2025-03-10T16:25:15.442116Z","shell.execute_reply":"2025-03-10T16:25:15.441950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mutation_dataset = mutation_dataset[mutation_dataset['Chromosome'] != 'MT']\nmutation_dataset = mutation_dataset[mutation_dataset['Chromosome'] != 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.444217Z","iopub.status.idle":"2025-03-10T16:25:15.444597Z","shell.execute_reply":"2025-03-10T16:25:15.444429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mutation_dataset = mutation_dataset.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.445448Z","iopub.status.idle":"2025-03-10T16:25:15.445826Z","shell.execute_reply":"2025-03-10T16:25:15.445660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mutation_dataset = mutation_dataset[mutation_dataset['Reference'].apply(len) < 21]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.446741Z","iopub.status.idle":"2025-03-10T16:25:15.447129Z","shell.execute_reply":"2025-03-10T16:25:15.446960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmutation_dataset = mutation_dataset[mutation_dataset['Chromosome'] != 3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.447927Z","iopub.status.idle":"2025-03-10T16:25:15.448324Z","shell.execute_reply":"2025-03-10T16:25:15.448153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_len = mutation_dataset['Reference'].apply(len).max()\nmax_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.452091Z","iopub.status.idle":"2025-03-10T16:25:15.452503Z","shell.execute_reply":"2025-03-10T16:25:15.452327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chromosome_dict = {str(key): value for key, value in chromosome_dict.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T09:17:57.672428Z","iopub.execute_input":"2025-03-22T09:17:57.672845Z","iopub.status.idle":"2025-03-22T09:17:57.678496Z","shell.execute_reply.started":"2025-03-22T09:17:57.672817Z","shell.execute_reply":"2025-03-22T09:17:57.677230Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# chromosome_dict","metadata":{"id":"ZokXGDdWRF8p","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.454543Z","iopub.status.idle":"2025-03-10T16:25:15.454874Z","shell.execute_reply":"2025-03-10T16:25:15.454747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mutation_dataset = pd.read_csv('/kaggle/input/mut-flank-csv/mut_flank.csv')\n# mutation_dataset = mutation_dataset.dropna(subset=['flanking_seq'])\n\n# # Вторая хромосома не вошла в словарь хромосом, потому фланкирующие последовательности для нее не отобраны\n# # Выбрасываем строки содержащие в 4 колонке Nan - это все значеения для 2 хромосомы\n\n# chromosome_counts = mutation_dataset['Chromosome'].value_counts(normalize=True)\n# # chromosome_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:05:45.396428Z","iopub.execute_input":"2025-03-22T13:05:45.396966Z","iopub.status.idle":"2025-03-22T13:05:45.401874Z","shell.execute_reply.started":"2025-03-22T13:05:45.396924Z","shell.execute_reply":"2025-03-22T13:05:45.400711Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# chromosome_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T14:27:42.974450Z","iopub.execute_input":"2025-03-22T14:27:42.974962Z","iopub.status.idle":"2025-03-22T14:27:42.979792Z","shell.execute_reply.started":"2025-03-22T14:27:42.974915Z","shell.execute_reply":"2025-03-22T14:27:42.978589Z"}},"outputs":[],"execution_count":132},{"cell_type":"code","source":"chromosome_lengths = {\n    chrom_key: len(hg38_genome[chrom_value]) \n    for chrom_key, chrom_value in chromosome_dict.items() \n    if chrom_value in hg38_genome.keys()  # Убедимся, что хромосома существует в genome\n }\n# chromosome_lengths = {\n#     str(chrom_key): chrom_length \n#     for chrom_key, chrom_length in chromosome_lengths.items()\n# }\n# # Проверим результат\nprint(chromosome_lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T11:18:22.391481Z","iopub.execute_input":"2025-03-22T11:18:22.391860Z","iopub.status.idle":"2025-03-22T11:18:22.399245Z","shell.execute_reply.started":"2025-03-22T11:18:22.391834Z","shell.execute_reply":"2025-03-22T11:18:22.397472Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"{1: 248956422, 2: 242193529, 3: 198295559, 4: 190214555, 5: 181538259, 6: 170805979, 7: 159345973, 8: 145138636, 9: 138394717, 10: 133797422, 11: 135086622, 12: 133275309, 13: 114364328, 14: 107043718, 15: 101991189, 16: 90338345, 17: 83257441, 18: 80373285, 19: 58617616, 20: 64444167, 21: 46709983, 22: 50818468, 'X': 156040895, 'Y': 57227415}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# chrom_length = chromosome_lengths[chrom]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:05:16.888688Z","iopub.execute_input":"2025-03-22T13:05:16.889264Z","iopub.status.idle":"2025-03-22T13:05:16.894455Z","shell.execute_reply.started":"2025-03-22T13:05:16.889226Z","shell.execute_reply":"2025-03-22T13:05:16.892641Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"### <span style=\"color: blue;\">Создаем список последовательностей нуклеотидов из референсного генома, выбрав такие участки, которые не перекрываются с позициями из датасета COSMIC.</span>\n","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nforbidden_regions = {}\n\n# Группируем данные по хромосомам\nfor chrom, group in tqdm(mutation_dataset.groupby('Chromosome')):\n    forbidden_regions[chrom] = [(pos - 200, pos + 200) for pos in group['Position']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T12:21:09.348569Z","iopub.execute_input":"2025-03-22T12:21:09.348977Z","iopub.status.idle":"2025-03-22T12:21:33.802659Z","shell.execute_reply.started":"2025-03-22T12:21:09.348946Z","shell.execute_reply":"2025-03-22T12:21:33.801024Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"100%|██████████| 23/23 [00:18<00:00,  1.22it/s]\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"\ndef get_free_intervals(chrom, forbidden_regions, chrom_length, min_length=201):\n    \"\"\"\n    Получает список свободных интервалов на хромосоме, которые не перекрывают запрещенные регионы\n    и имеют длину не менее min_length.\n    \"\"\"\n    # Сортируем запрещенные регионы по начальной позиции\n    forbidden_regions_sorted = sorted(forbidden_regions, key=lambda x: x[0])\n    \n    # Инициализируем список свободных интервалов\n    free_intervals = []\n    \n    # Начальный интервал от 0 до начала первого запрещенного региона\n    prev_end = 0\n    \n    for start, end in forbidden_regions_sorted:\n        # Если текущий запрещенный регион не перекрывает предыдущий, то мы находим свободный интервал\n        if start > prev_end:\n            free_intervals.append((prev_end, start))\n        prev_end = max(prev_end, end)\n    \n    # Добавляем последний интервал от конца последнего запрещенного региона до конца хромосомы\n    if prev_end < chrom_length:\n        free_intervals.append((prev_end, chrom_length))\n    \n    # Фильтруем свободные интервалы по минимальной длине\n    free_intervals = [interval for interval in free_intervals if interval[1] - interval[0] >= min_length]\n    \n    return free_intervals\n\n# Статистика для первой хромосомы\nchrom = 1  # Пример для хромосомы 1\nchrom_length = chromosome_lengths.get(chrom)  # Получаем длину хромосомы\nforbidden = forbidden_regions.get(chrom, [])  # Получаем список запрещенных регионов\n\n# Получаем свободные интервалы\nfree_intervals = get_free_intervals(chrom, forbidden, chrom_length)\n\n# Выводим статистику\nprint(f\"Хромосома {chrom}, длина: {chrom_length}\")\nprint(f\"Количество свободных интервалов длиной не менее 201 базы: {len(free_intervals)}\")\n\n# Проверим количество разрешенных регионов для каждой хромосомы\nfor chrom in allowed_regions:\n    print(f\"Хромосома {chrom}: {len(allowed_regions[chrom])} разрешенных интервалов\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:36:25.636658Z","iopub.execute_input":"2025-03-22T13:36:25.637291Z","iopub.status.idle":"2025-03-22T13:36:25.651398Z","shell.execute_reply.started":"2025-03-22T13:36:25.637247Z","shell.execute_reply":"2025-03-22T13:36:25.649769Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Хромосома 1: 58258 разрешенных интервалов\nХромосома 2: 41868 разрешенных интервалов\nХромосома 4: 36897 разрешенных интервалов\nХромосома 5: 33949 разрешенных интервалов\nХромосома 6: 36704 разрешенных интервалов\nХромосома 7: 33631 разрешенных интервалов\nХромосома 8: 24992 разрешенных интервалов\nХромосома 9: 26378 разрешенных интервалов\nХромосома 10: 33226 разрешенных интервалов\nХромосома 11: 31855 разрешенных интервалов\nХромосома 12: 32393 разрешенных интервалов\nХромосома 13: 16684 разрешенных интервалов\nХромосома 14: 22366 разрешенных интервалов\nХромосома 15: 24295 разрешенных интервалов\nХромосома 16: 20420 разрешенных интервалов\nХромосома 17: 25245 разрешенных интервалов\nХромосома 18: 16042 разрешенных интервалов\nХромосома 19: 16639 разрешенных интервалов\nХромосома 20: 14810 разрешенных интервалов\nХромосома 21: 6581 разрешенных интервалов\nХромосома 22: 12297 разрешенных интервалов\nХромосома X: 29573 разрешенных интервалов\nХромосома Y: 1617 разрешенных интервалов\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"# Создаем словарь для разрешенных регионов\nallowed_regions = {}\n\n# Группируем данные по хромосомам\nfor chrom, group in tqdm(mutation_dataset.groupby('Chromosome')):\n    forbidden = forbidden_regions.get(chrom, [])\n    chrom_length = chromosome_lengths.get(chrom)\n\n    # Находим все возможные свободные участки (разрешенные регионы) между запрещенными регионами\n    allowed = []\n    prev_end = 0\n    for start, end in sorted(forbidden):\n        if start > prev_end:  # Если есть промежуток между запрещенными регионами\n            allowed.append((prev_end, start))  # Добавляем свободный интервал\n        prev_end = max(prev_end, end)  # Обновляем конец последнего запрещенного региона\n\n    # Добавляем оставшийся участок после последнего запрещенного региона, если таковой имеется\n    if prev_end < chrom_length:\n        allowed.append((prev_end, chrom_length))\n    \n    allowed_regions[chrom] = allowed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T12:53:02.273695Z","iopub.execute_input":"2025-03-22T12:53:02.274201Z","iopub.status.idle":"2025-03-22T12:53:27.173075Z","shell.execute_reply.started":"2025-03-22T12:53:02.274168Z","shell.execute_reply":"2025-03-22T12:53:27.171735Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"100%|██████████| 23/23 [00:21<00:00,  1.06it/s]\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# Сгенерируем последовательности нуклеотидов из разрешенных участков хромосом, которые не перекрываются \n# с запрещенными - позиции неклоетидных замен из датасета Cosmic +-200пн\n\nrandom_sites = []\ntotal_samples = 1_000_000\nnan_warning_shown = False\n\nwith tqdm(total=total_samples, desc=\"Generating random sites\", ncols=100) as pbar:\n    while len(random_sites) < total_samples:\n        for chrom, proportion in chromosome_counts.items():\n            hg38_key = chromosome_dict.get(chrom)\n            if hg38_key not in hg38_genome:\n                continue\n\n            num_samples = int(proportion * total_samples)\n            forbidden = forbidden_regions.get(chrom, [])\n            allowed = allowed_regions.get(chrom, [])\n            \n            for _ in range(num_samples):\n                max_attempts = 100\n                attempts = 0\n                while attempts < max_attempts:\n                    # Выбираем случайный разрешенный регион\n                    start_pos, end_pos = random.choice(allowed)\n                    if end_pos - start_pos < 201:\n                        continue  # Пропускаем, если интервал слишком короткий\n                    \n                    # Генерируем случайный участок внутри разрешенного региона\n                    start_pos = random.randint(start_pos, end_pos - 201)\n                    end_pos = start_pos + 201\n\n                    sequence = hg38_genome[hg38_key][start_pos:end_pos].upper()\n\n                    # Проверка на 'N' в последовательности\n                    if 'N' in sequence:\n                        attempts += 1\n                        continue\n                    \n                    # Проверка на NaN\n                    if pd.isna(sequence):\n                        if not nan_warning_shown:\n                            print(f\"⚠️ ВНИМАНИЕ: Найдено NaN в последовательности на хромосоме {chrom}, позиции {start_pos}-{end_pos}\")\n                            nan_warning_shown = True\n                        attempts += 1\n                        continue\n\n                    # Добавление валидного участка\n                    random_sites.append({'Chromosome': chrom, 'Start': start_pos, 'End': end_pos, 'Sequence': sequence})\n                    break\n\n                if attempts >= max_attempts:\n                    print(f\"Не удалось сгенерировать участок для хромосомы {chrom} после {max_attempts} попыток.\")\n\n            pbar.update(len(random_sites) - pbar.n)\n\n# Создаем DataFrame\nrandom_seq = pd.DataFrame(random_sites)\n\nprint(f\"✅ Сгенерировано {len(random_seq)} случайных участков.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:51:45.521287Z","iopub.execute_input":"2025-03-22T13:51:45.521726Z","iopub.status.idle":"2025-03-22T13:52:28.241079Z","shell.execute_reply.started":"2025-03-22T13:51:45.521695Z","shell.execute_reply":"2025-03-22T13:52:28.239863Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"Generating random sites: 1999976it [00:40, 49828.33it/s]                                            \n","output_type":"stream"},{"name":"stdout","text":"✅ Сгенерировано 1999976 случайных участков.\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"# random_seq['Sequence'] = random_seq['Sequence'].apply(lambda x: \"\".join(x))\n# random_seq[\"Position\"] = (random_seq[\"Start\"] + random_seq[\"End\"]) // 2\n# random_seq[\"ID\"] = 'REFA00000001'\n# random_seq = random_seq.drop(columns = ['Start', 'End'])\n# random_seq = random_seq[[\"Chromosome\", \"Position\", \"ID\", \"Reference\", \"Sequence\"]]\n# random_seq.rename(columns = {'Sequence':'flanking_seq'}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T14:08:53.117285Z","iopub.execute_input":"2025-03-22T14:08:53.117690Z","iopub.status.idle":"2025-03-22T14:08:53.123187Z","shell.execute_reply.started":"2025-03-22T14:08:53.117658Z","shell.execute_reply":"2025-03-22T14:08:53.121990Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"random_seq.to_csv('random_sequence.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T14:09:08.857046Z","iopub.execute_input":"2025-03-22T14:09:08.857420Z","iopub.status.idle":"2025-03-22T14:09:19.722300Z","shell.execute_reply.started":"2025-03-22T14:09:08.857390Z","shell.execute_reply":"2025-03-22T14:09:19.721101Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"df = pd.concat([random_seq, mut_dataset], ignore_index=True)\n# Добавляем лейблы для референсных последовательнотей и Cosmic данных как 0 и 1 \ndf['label'] = df['Reference'].apply(lambda x: 0 if x == 'R' else 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T14:09:26.142149Z","iopub.execute_input":"2025-03-22T14:09:26.142562Z","iopub.status.idle":"2025-03-22T14:09:26.922056Z","shell.execute_reply.started":"2025-03-22T14:09:26.142531Z","shell.execute_reply":"2025-03-22T14:09:26.920730Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"df.to_csv('cosimic_reference_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T14:17:20.748395Z","iopub.execute_input":"2025-03-22T14:17:20.748823Z","iopub.status.idle":"2025-03-22T14:17:53.131641Z","shell.execute_reply.started":"2025-03-22T14:17:20.748790Z","shell.execute_reply":"2025-03-22T14:17:53.130243Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"# # Создаем датасет с 3 нужными колонками \"COSMIC_SAMPLE_ID\", \"COSMIC_PHENOTYPE_ID\", \"GENOMIC_MUTATION_ID\"\n# # из файла со всеми однонуклеотидными мутациями и сохраняем в отлельный датасет\n# # Для анализа кластеров полученных после автоэнкодера\n\n# input_file = \"/kaggle/input/cosmic-genomescreensmutant-tsv-v101-grch38-tar/Cosmic_GenomeScreensMutant_v101_GRCh38.tsv/Cosmic_GenomeScreensMutant_v101_GRCh38.tsv\"\n# output_file = \"phenotype_id.tsv\"\n# columns_to_extract = [\"COSMIC_SAMPLE_ID\", \"COSMIC_PHENOTYPE_ID\", \"GENOMIC_MUTATION_ID\"]  # Заменить на реальные названия\n\n# chunksize = 5000000\n\n# with pd.read_csv(input_file, sep='\\t', chunksize=chunksize, usecols=columns_to_extract) as reader:\n#     for i, chunk in enumerate(reader):\n#         chunk.to_csv(output_file, sep='\\t', mode='a', index=False, header=(i == 0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.461199Z","iopub.status.idle":"2025-03-10T16:25:15.461441Z","shell.execute_reply":"2025-03-10T16:25:15.461342Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **AUTOENCODER**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/reduced-mut-dataset/reduced_mut_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.462116Z","iopub.status.idle":"2025-03-10T16:25:15.462394Z","shell.execute_reply":"2025-03-10T16:25:15.462294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df = df[df['Chromosome'].isin([1,2,4,5,6,7,8,9,10,11,12,14,15,16,17,19,'X'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.463160Z","iopub.status.idle":"2025-03-10T16:25:15.463606Z","shell.execute_reply":"2025-03-10T16:25:15.463393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pheno = pd.read_csv('/kaggle/input/phenotype/phenotype_id.tsv', sep='\\t')\nclassification = pd.read_csv('/kaggle/input/cosmic-classification-v101-grch38-tsv/Cosmic_Classification_v101_GRCh38.tsv', sep='\\t')\n\ncarcinoma = classification[classification['PRIMARY_HISTOLOGY'].isin(['carcinoma', 'other', 'glioma'])]\nfiltered_pheno = pheno[pheno['COSMIC_PHENOTYPE_ID'].isin(carcinoma['COSMIC_PHENOTYPE_ID'])]\nfiltered_pheno = filtered_pheno.merge(carcinoma[['COSMIC_PHENOTYPE_ID', 'PRIMARY_HISTOLOGY']], \n                                      on='COSMIC_PHENOTYPE_ID', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.464229Z","iopub.status.idle":"2025-03-10T16:25:15.464475Z","shell.execute_reply":"2025-03-10T16:25:15.464373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df_test = df[df['Chromosome'].isin([17, 13])]\n# df = df.groupby('Chromosome').sample(frac=0.3, random_state=42)\n# df['flanking_seq'] = df['flanking_seq'].str.slice(70, -70)\ndf['flanking_seq'] = df['flanking_seq'].str[:-1]\n# df['flanking_seq'] = df['flanking_seq'].str.slice(85, -85)\nmax = df['flanking_seq'].apply(len).max()\nmax","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.465223Z","iopub.status.idle":"2025-03-10T16:25:15.465565Z","shell.execute_reply":"2025-03-10T16:25:15.465398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter df using updated filtered_pheno\ndf_filtered = df[df['ID'].isin(filtered_pheno['GENOMIC_MUTATION_ID'])].merge(\n    filtered_pheno[['GENOMIC_MUTATION_ID', 'PRIMARY_HISTOLOGY']], \n    left_on='ID', right_on='GENOMIC_MUTATION_ID', how='left'\n).drop(columns=['GENOMIC_MUTATION_ID'])\n\n# Reduce carcinoma cases by 10x\ndf_filtered = pd.concat([\n    df_filtered[df_filtered['PRIMARY_HISTOLOGY'] == 'carcinoma'].sample(frac=0.1, random_state=98),\n    df_filtered[df_filtered['PRIMARY_HISTOLOGY'] != 'carcinoma']\n], ignore_index=True)\ndf_filtered['PRIMARY_HISTOLOGY'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.466319Z","iopub.status.idle":"2025-03-10T16:25:15.466554Z","shell.execute_reply":"2025-03-10T16:25:15.466457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_valid = train_test_split(df_filtered, test_size = 0.2, random_state = 44, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.467405Z","iopub.status.idle":"2025-03-10T16:25:15.467719Z","shell.execute_reply":"2025-03-10T16:25:15.467608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train = X_train.groupby('Chromosome').sample(frac=0.1, random_state=39)\n#X_valid = X_valid.groupby('Chromosome').sample(frac=0.1, random_state=42)\nX_val, X_test = train_test_split(X_valid, test_size = 0.1, random_state = 46)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.468468Z","iopub.status.idle":"2025-03-10T16:25:15.468716Z","shell.execute_reply":"2025-03-10T16:25:15.468612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#X_val = X_val.groupby('Chromosome').sample(frac=0.15, random_state=12)\nX_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.469247Z","iopub.status.idle":"2025-03-10T16:25:15.469476Z","shell.execute_reply":"2025-03-10T16:25:15.469382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counts = X_test['Chromosome'].value_counts()\n\nfiltered_counts = counts[counts > 500].index\nnot_filtered_counts = counts[counts < 501].index\n\nfiltered_data = X_test[X_test['Chromosome'].isin(filtered_counts)]\nnot_filtered_data = X_test[X_test['Chromosome'].isin(not_filtered_counts)]\n\nreduced_filtered_data = filtered_data.groupby('Chromosome').sample(frac=0.3, random_state=41)\n\ntest_data = pd.concat([reduced_filtered_data, not_filtered_data])\n\ntest_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.469953Z","iopub.status.idle":"2025-03-10T16:25:15.470220Z","shell.execute_reply":"2025-03-10T16:25:15.470091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nucleotide_to_index = {\n    'A': 0,\n    'C': 1,\n    'G': 2,\n    'T': 3\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.471048Z","iopub.status.idle":"2025-03-10T16:25:15.471551Z","shell.execute_reply":"2025-03-10T16:25:15.471393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Обуаемый эмбеддинг\ndef sequence_to_indices(sequence):\n    try:\n        indices = np.array(list(sequence))  # Преобразуем строку в массив символов\n        return np.vectorize(nucleotide_to_index.get)(indices)  # Применяем словарь для преобразования символов в индексы\n    except Exception as e:\n        # Выводим отладочную информацию\n        print(f\"Ошибка в последовательности: {sequence}\")\n        raise e  # Повторно выбрасываем исключение для диагностики\n\nX_train['indexed_flanking_seq'] = X_train['flanking_seq'].apply(sequence_to_indices)\nX_val['indexed_flanking_seq'] = X_val['flanking_seq'].apply(sequence_to_indices)\ntest_data['indexed_flanking_seq'] = test_data['flanking_seq'].apply(sequence_to_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.472340Z","iopub.status.idle":"2025-03-10T16:25:15.472630Z","shell.execute_reply":"2025-03-10T16:25:15.472515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Функция для генерации последовательности длиной 201 для каждого из символов\n# def generate_sequence(base, length=201):\n#     return base * (length // len(base)) + base[:length % len(base)]\n\n# # Генерация \n# num_sequences = 10000\n# bases = ['A', 'C', 'T', 'G']\n# sequences = []\n\n# for _ in range(num_sequences):\n#     for base in bases:\n#         sequences.append(generate_sequence(base))\n\n# # Печать первых 5 последовательностей для проверки\n# for seq in sequences[:5]:\n#     print(seq)\n\n# sequences_series = pd.Series(sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.473444Z","iopub.status.idle":"2025-03-10T16:25:15.473751Z","shell.execute_reply":"2025-03-10T16:25:15.473595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Функция для генерации случайной последовательности длиной 200 с вариациями\n# def generate_random_sequence(length=200, variation_rate=0.1):\n#     bases = ['A', 'C', 'T', 'G']\n#     sequence = [random.choice(bases) for _ in range(length)]\n#     num_variations = int(length * variation_rate)\n\n#     for _ in range(num_variations):\n#         position = random.randint(0, length - 1)\n#         new_base = random.choice([b for b in bases if b != sequence[position]])\n#         sequence[position] = new_base\n    \n#     return ''.join(sequence)\n\n# # Генерация cлучайных последовательностей с вариациями\n# num_sequences = 10000\n# random_sequences = []\n\n# for _ in range(num_sequences):\n#     sequence = generate_random_sequence()\n#     random_sequences.append(sequence)\n\n# # Печать первых 5 случайных последовательностей для проверки\n# for seq in sequences[:5]:\n#     print(seq)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.475025Z","iopub.status.idle":"2025-03-10T16:25:15.475361Z","shell.execute_reply":"2025-03-10T16:25:15.475210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import random\n\n# # Функция для генерации случайной последовательности с заданным GC-составом\n# def generate_sequence_with_gc_content(length=200, gc_content=0.6):\n#     bases_gc = ['G', 'C']\n#     bases_at = ['A', 'T']\n    \n#     # Определяем количество нуклеотидов GC и AT\n#     num_gc = int(length * gc_content)\n#     num_at = length - num_gc\n\n#     # Генерация частей последовательности\n#     gc_sequence = [random.choice(bases_gc) for _ in range(num_gc)]\n#     at_sequence = [random.choice(bases_at) for _ in range(num_at)]\n\n#     # Перемешиваем и создаем полную последовательность\n#     full_sequence = gc_sequence + at_sequence\n#     random.shuffle(full_sequence)\n\n#     return ''.join(full_sequence)\n\n# # Функция для генерации последовательностей с разными GC-составами\n# def generate_sequences_with_varied_gc(length=200, gc_contents=[0.6, 0.4], num_sequences=10000):\n#     sequences = []\n#     for _ in range(num_sequences):\n#         gc_content = random.choice(gc_contents)  # Выбираем случайный GC-состав из списка\n#         sequence = generate_sequence_with_gc_content(length, gc_content)\n#         sequences.append(sequence)\n    \n#     return sequences\n\n# # Генерация 10 000 последовательностей с GC-составом 60% и 40%\n# num_sequences = 10000\n# gc_contents = [0.6, 0.4]\n# random_sequences = generate_sequences_with_varied_gc(length=200, gc_contents=gc_contents, num_sequences=num_sequences)\n# random_sequences = pd.DataFrame(random_sequences)\n# # Печать первых 5 последовательностей для проверки\n# for seq in random_sequences[:5]:\n#     print(seq)\n# random_sequences['indexed_flanking_seq'] = random_sequences[0].apply(sequence_to_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.476301Z","iopub.status.idle":"2025-03-10T16:25:15.476607Z","shell.execute_reply":"2025-03-10T16:25:15.476452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def sequence_to_indices(sequence):\n#     # Define a dictionary mapping nucleotides to indices\n#     nucleotide_to_index = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    \n#     # Convert the sequence to a list of indices\n#     indices = [nucleotide_to_index[nucleotide] for nucleotide in sequence]\n    \n#     return indices\n\n# # Apply the function to the DataFrame\n# random_sequences['indexed_flanking_seq'] = random_sequences[0].apply(sequence_to_indices)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.477382Z","iopub.status.idle":"2025-03-10T16:25:15.477697Z","shell.execute_reply":"2025-03-10T16:25:15.477542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# random_sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.478690Z","iopub.status.idle":"2025-03-10T16:25:15.479051Z","shell.execute_reply":"2025-03-10T16:25:15.478880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define k sizes\nk_sizes = [3]  # You can extend this list for multiple k sizes\n\n# Generate all possible k-mers with len k_size\nall_kmers = [''.join(p) for k in k_sizes for p in product('ACGT', repeat=k)]\n\n# Create k-mer to integer mapping\nkmer_to_int = {kmer: idx for idx, kmer in enumerate(all_kmers)}\n\n                     \n# Function to generate k-mers from my sequence\ndef generate_variable_kmers(sequence, k_sizes):\n   kmers = []\n   for k in k_sizes:\n       kmers.extend([''.join(sequence[i:i+k]) for i in range(len(sequence) - k + 1)])\n   return kmers\n\n# Function to encode k-mers using the predefined mapping\ndef integer_encode_variable_kmers(sequence, k_sizes, kmer_to_int):\n   kmers = generate_variable_kmers(sequence, k_sizes)\n   return [kmer_to_int[kmer] for kmer in kmers if kmer in kmer_to_int]\n\n# Apply integer encoding to the flanking sequences\nX_train['encoded_kmers'] = X_train['flanking_seq'].apply(lambda seq: integer_encode_variable_kmers(seq, k_sizes, kmer_to_int))\nX_val['encoded_kmers'] = X_val['flanking_seq'].apply(lambda seq: integer_encode_variable_kmers(seq, k_sizes, kmer_to_int))\ntest_data['encoded_kmers'] = test_data['flanking_seq'].apply(lambda seq: integer_encode_variable_kmers(seq, k_sizes, kmer_to_int))\n#print(X_train[['flanking_seq', 'encoded_kmers']])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.479742Z","iopub.status.idle":"2025-03-10T16:25:15.480032Z","shell.execute_reply":"2025-03-10T16:25:15.479931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max = X_train['encoded_kmers'].apply(len).max()\nmax","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.481319Z","iopub.status.idle":"2025-03-10T16:25:15.481587Z","shell.execute_reply":"2025-03-10T16:25:15.481482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nucleotide_dim = 4 #len(kmer_to_int)\nembedding_dim = 64\nlatent_dim = 16\n\nlearning_rate = 0.002\n\nbatch_size = 64\nnum_epochs = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.482291Z","iopub.status.idle":"2025-03-10T16:25:15.482592Z","shell.execute_reply":"2025-03-10T16:25:15.482436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NucleotideAutoencoder(nn.Module):\n    def __init__(self, nucleotide_dim=nucleotide_dim, embedding_dim=embedding_dim):\n        super(NucleotideAutoencoder, self).__init__()\n\n        self.embedding = nn.Embedding(num_embeddings=nucleotide_dim, embedding_dim=embedding_dim)\n\n        self.encoder = nn.Sequential(\n            nn.Conv1d(embedding_dim, 32, kernel_size=3, padding=1),\n            nn.SiLU(),\n            nn.MaxPool1d(2),\n            # nn.Conv1d(128, 64, kernel_size=3, padding=1),\n            # nn.SiLU(),\n            # nn.Dropout(0.2),\n            # nn.MaxPool1d(2),\n            nn.Conv1d(32, 16, kernel_size=3, padding=1),\n            nn.SiLU(),\n            nn.MaxPool1d(2)\n        )\n        \n        # Flatten and fully connected layers for latent features\n        self.flatten = nn.Flatten()\n        self.fc_latent = nn.Linear(16 * (200 // 4), latent_dim)  # x // y — длина последовательности после 3 MaxPool\n\n        \n        # Decoder\n        self.fc_restore = nn.Linear(latent_dim, 16 * (200 // 4))\n        self.unflatten = nn.Unflatten(1, (16, 200 // 4))\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose1d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.SiLU(),\n            # nn.ConvTranspose1d(64, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            # nn.SiLU(),\n            nn.ConvTranspose1d(32, embedding_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.SiLU()\n        )\n        \n    def encode(self, x):\n        \"\"\"\n        Encodes the input sequence to its latent representation.\n        Args:\n            x (Tensor): Input sequence (batch_size, sequence_length).\n        Returns:\n            Tensor: Latent representation (batch_size, latent_dim).\n        \"\"\"\n        x = self.embedding(x) \n        x = x.permute(0, 2, 1)  \n        x = self.encoder(x)  \n        x = self.flatten(x)  \n        latent = self.fc_latent(x)  \n        return latent\n\n    def forward(self, x):\n        #print(f\"Shape before embedding layer: {x.shape}\")\n        x = self.embedding(x)  # Map nucleotides to embeddings\n        #print(f\"Shape after embedding layer: {x.shape}\") \n        x = x.permute(0, 2, 1)  # Reshape for Conv1d (batch_size, embedding_dim, sequence_length)\n        #print(f\"Shape after permute layer: {x.shape}\") \n        x = self.encoder(x)\n        #print(f\"Shape after encoder layer: {x.shape}\") \n        x = self.flatten(x)\n        #print(f\"Shape after flatten layer: {x.shape}\") \n        x = self.fc_latent(x)\n        #print(f\"Shape after fc_latent layer: {x.shape}\") \n        x = self.fc_restore(x)\n        #print(f\"Shape after fc_restore layer: {x.shape}\") \n        x = self.unflatten(x)\n        #print(f\"Shape after unflatten layer: {x.shape}\") \n        x = self.decoder(x)\n        #print(f\"Shape after decoder layer: {x.shape}\") \n        x = x.permute(0, 2, 1)  # Restore original shape for output layer\n        #print(f\"Shape after permute layer: {x.shape}\")   \n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.483298Z","iopub.status.idle":"2025-03-10T16:25:15.483550Z","shell.execute_reply":"2025-03-10T16:25:15.483448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_encoded = torch.tensor(list(X_train['indexed_flanking_seq'].values), dtype=torch.long)\nX_val_encoded = torch.tensor(list(X_val['indexed_flanking_seq'].values), dtype = torch.long)\nX_test_encoded = torch.tensor(list(test_data['indexed_flanking_seq'].values), dtype = torch.long)\n\n#X_train_encoded = torch.tensor(list(X_train['encoded_kmers'].values), dtype=torch.long)\n#X_val_encoded = torch.tensor(list(X_val['encoded_kmers'].values), dtype = torch.long)\n#X_test_encoded = torch.tensor(list(test_data['encoded_kmers'].values), dtype = torch.long)\n\ntrain_dataset = TensorDataset(X_train_encoded)\nval_dataset = TensorDataset(X_val_encoded)\ntest_dataset = TensorDataset(X_test_encoded)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.484997Z","iopub.status.idle":"2025-03-10T16:25:15.485312Z","shell.execute_reply":"2025-03-10T16:25:15.485187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"autoencoder = NucleotideAutoencoder(nucleotide_dim=nucleotide_dim, embedding_dim=embedding_dim)\noptimizer = optim.Adam(autoencoder.parameters(), lr = learning_rate)\ncriterion = nn.MSELoss()\n\n# Move the model to the GPUs\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = xm.xla_device()\n# print(f\"Using device: {xm.xla_device()}\")\nautoencoder = autoencoder.to(device)\n\n# # Use DataParallel to utilize multiple GPUs\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n#     autoencoder = nn.DataParallel(autoencoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:33:01.803034Z","iopub.execute_input":"2025-03-10T16:33:01.803424Z","iopub.status.idle":"2025-03-10T16:33:01.814468Z","shell.execute_reply.started":"2025-03-10T16:33:01.803396Z","shell.execute_reply":"2025-03-10T16:33:01.813490Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"train_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    autoencoder.train()\n    train_loss = 0\n    for batch in train_dataloader:\n        batch = batch[0].to(device)\n        optimizer.zero_grad()\n        \n        outputs = autoencoder(batch).to(device)\n        batch = batch.float().unsqueeze(-1).repeat(1, 1, embedding_dim)\n        \n        loss = criterion(outputs, batch)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_dataloader)\n    train_losses.append(train_loss)\n\n    autoencoder.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            batch = batch[0].to(device)\n            outputs = autoencoder(batch).to(device)\n            batch = batch.float().unsqueeze(-1).repeat(1, 1, embedding_dim)\n        \n            loss = criterion(outputs, batch)\n            val_loss += loss.item()\n\n    val_loss /= len(val_dataloader)\n    val_losses.append(val_loss)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.487210Z","iopub.status.idle":"2025-03-10T16:25:15.487452Z","shell.execute_reply":"2025-03-10T16:25:15.487348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_test_encoded = torch.tensor(list(test_data['indexed_flanking_seq'].values), dtype = torch.long)\n# test_dataset = TensorDataset(X_test_encoded)\n# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.488926Z","iopub.status.idle":"2025-03-10T16:25:15.489185Z","shell.execute_reply":"2025-03-10T16:25:15.489059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# random_seq_encoded = torch.tensor(list(random_sequences['indexed_flanking_seq'].values), dtype = torch.long)\n# random_dataset = TensorDataset(random_seq_encoded)\n# random_datatloader = DataLoader(random_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.489698Z","iopub.status.idle":"2025-03-10T16:25:15.489929Z","shell.execute_reply":"2025-03-10T16:25:15.489830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize a list to store encoded features\nencoded_features_list = []\n\n# Disable gradient calculation for inference\nwith torch.no_grad():\n    for batch in test_dataloader:\n        # Extract input sequences from the batch\n        input_sequence = batch[0].long().to(device)  # Replace 'sequence' with the actual key in your dataset\n\n        # Pass input to the encoder\n        encoded_features = autoencoder.encode(input_sequence).to(device)\n\n        # Append encoded features to the list\n        encoded_features_list.append(encoded_features)\n\n# Concatenate all encoded features into a single tensor\nencoded_features_tensor = torch.cat(encoded_features_list, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.490868Z","iopub.status.idle":"2025-03-10T16:25:15.491197Z","shell.execute_reply":"2025-03-10T16:25:15.491057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_features_tensor = encoded_features_tensor.cpu()\nfor i in range(encoded_features_tensor.shape[1]):  # Loop over each latent dimension\n    test_data[f'encoded_feature_{i + 1}'] = encoded_features_tensor[:, i]\n    #X_val[f'encoded_feature_{i + 1}'] = encoded_features_tensor[:, i]\n    #X_valid[f'encoded_feature_{i + 1}'] = encoded_features_tensor[:, i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.491949Z","iopub.status.idle":"2025-03-10T16:25:15.492264Z","shell.execute_reply":"2025-03-10T16:25:15.492100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.to_csv('test_data_latent.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.493080Z","iopub.status.idle":"2025-03-10T16:25:15.493382Z","shell.execute_reply":"2025-03-10T16:25:15.493267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_features_tensor = encoded_features_tensor[:22000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.494353Z","iopub.status.idle":"2025-03-10T16:25:15.494644Z","shell.execute_reply":"2025-03-10T16:25:15.494533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_features_tensor.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.495315Z","iopub.status.idle":"2025-03-10T16:25:15.495553Z","shell.execute_reply":"2025-03-10T16:25:15.495458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n#encoded_features_channels_sequence = np.median(encoded_features_np, axis=1)\ndbscan_labels = dbscan.fit_predict(encoded_features_tensor)\nprint(\"Кластеры:\", np.unique(dbscan_labels))\nprint(\"Количество кластеров:\", len(np.unique(dbscan_labels)))\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nif len(np.unique(dbscan_labels)) > 1:\n    sil_score = silhouette_score(encoded_features_tensor, dbscan_labels)\n    print(f\"Silhouette Score: {sil_score:.2f}\")\nelse:\n    print(\"Недостаточно кластеров для вычисления Silhouette Score.\")\n\n# Уменьшение размерности до 2D для визуализации\npca = PCA(n_components=2)\nreduced_features = pca.fit_transform(encoded_features_tensor)\n\nplt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=dbscan_labels, cmap='viridis', marker='o')\nplt.title(\"Результаты кластеризации DBSCAN\")\nplt.xlabel(\"Компонента 1\")\nplt.ylabel(\"Компонента 2\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.496280Z","iopub.status.idle":"2025-03-10T16:25:15.496591Z","shell.execute_reply":"2025-03-10T16:25:15.496481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, metric='sqeuclidean')\nhdbscan_labels = hdbscan_clusterer.fit_predict(encoded_features_tensor)\n\nif len(np.unique(hdbscan_labels)) > 1:\n    sil_score = silhouette_score(encoded_features_tensor, hdbscan_labels)\n    print(f\"Silhouette Score: {sil_score:.2f}\")\nelse:\n    print(\"Недостаточно кластеров для вычисления Silhouette Score.\")\n\n\n# Вывод результатов\nprint(\"Кластеры:\", np.unique(hdbscan_labels))\nprint(\"Количество кластеров:\", len(np.unique(hdbscan_labels)))\nprint(\"Шумовые точки (помечены -1):\", np.sum(hdbscan_labels == -1))\n\n# Уменьшение размерности до *D для визуализации\npca = PCA(n_components=3)\nreduced_features = pca.fit_transform(encoded_features_tensor)\n\n# Визуализация 3D\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(\n    reduced_features[:, 0], \n    reduced_features[:, 1], \n    reduced_features[:, 2], \n    c=hdbscan_labels, \n    cmap='viridis',\n    marker='o',\n    alpha=0.6\n)\n\nax.set_title(\"Результаты кластеризации HDBSCAN в 3D\")\nax.set_xlabel(\"Компонента 1\")\nax.set_ylabel(\"Компонента 2\")\nax.set_zlabel(\"Компонента 3\")\nfig.colorbar(scatter, label=\"Кластеры\")\n\noutput_path = \"visualization_3D.png\" \nplt.savefig(output_path, dpi=300, bbox_inches='tight')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.497483Z","iopub.status.idle":"2025-03-10T16:25:15.497771Z","shell.execute_reply":"2025-03-10T16:25:15.497629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install leidenalg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.498621Z","iopub.status.idle":"2025-03-10T16:25:15.498921Z","shell.execute_reply":"2025-03-10T16:25:15.498804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install umap-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.499550Z","iopub.status.idle":"2025-03-10T16:25:15.499888Z","shell.execute_reply":"2025-03-10T16:25:15.499721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport igraph as ig\nfrom leidenalg import find_partition\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom leidenalg import ModularityVertexPartition\nfrom leidenalg import RBConfigurationVertexPartition, find_partition\nimport plotly.express as px\nimport umap\nfrom scipy.spatial.distance import cdist\nfrom sklearn.preprocessing import normalize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.500757Z","iopub.status.idle":"2025-03-10T16:25:15.501057Z","shell.execute_reply":"2025-03-10T16:25:15.500928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Применяем UMAP к латентным представлениям\numap_reducer = umap.UMAP(n_neighbors=495, min_dist=0.2, n_components=2, random_state=42)\numap_latent = umap_reducer.fit_transform(encoded_features_tensor)\n\n# 1. Construct the k-NN graph\nknn_graph = kneighbors_graph(umap_latent, n_neighbors=495, mode='connectivity', include_self=False)\n\n# 2. Build a similarity matrix (cosine similarity as an example)\nsimilarity_matrix = cosine_similarity(umap_latent)\n\n# Нормализация матрицы сходства\n#similarity_matrix = (similarity_matrix + 1) / 2\n\n# from scipy.spatial.distance import cdist\n# manhattan_distance_matrix = cdist(umap_latent, umap_latent, metric='cityblock')\n# manhattan_similarity = 1 / (1 + manhattan_distance_matrix)  # Преобразование расстояния в сходство\n\n# sources, targets = knn_graph.nonzero()\n# weights = manhattan_similarity[sources, targets]\n\n# Convert similarity matrix into graph format for Leiden\nsources, targets = knn_graph.nonzero()\nweights = similarity_matrix[sources, targets]\nweights = np.maximum(weights, 0) \nedges = list(zip(sources, targets))\ng = ig.Graph(edges=edges, directed=False)\n\n# 3. Run the Leiden algorithm\npartition = find_partition(\n    g,\n    partition_type=RBConfigurationVertexPartition,\n    weights=weights,\n    resolution_parameter=6.5  # Higher resolution for more clusters\n)\n\n# Extract cluster labels\nleiden_clusters = np.array(partition.membership)\n\nprint(\"Количество кластеров:\", len(np.unique(leiden_clusters)))\n\nif len(np.unique(leiden_clusters)) > 1:\n    sil_score = silhouette_score(umap_latent, leiden_clusters)\n    print(f\"Silhouette Score: {sil_score:.2f}\")\nelse:\n    print(\"Недостаточно кластеров для вычисления Silhouette Score.\")\n\n\n# 4. Visualize the Leiden clusters\n# Perform PCA for 2D visualization\npca = PCA(n_components=2)\nlatent_2d = pca.fit_transform(umap_latent)\n\n# Plot\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=leiden_clusters, cmap='seismic', s=10, alpha=0.8)\nplt.colorbar(scatter, label=\"Leiden Clusters\")\nplt.title(\"Leiden Clustering on Latent Representations\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.show()\n\n# Визуализация результатов UMAP\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(umap_latent[:, 0], umap_latent[:, 1], c=leiden_clusters, cmap='coolwarm', s=10, alpha=0.8)\nplt.colorbar(scatter, label=\"Leiden Clusters\")\nplt.title(\"Leiden Clustering with UMAP\")\nplt.xlabel(\"UMAP1\")\nplt.ylabel(\"UMAP2\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.502002Z","iopub.status.idle":"2025-03-10T16:25:15.502300Z","shell.execute_reply":"2025-03-10T16:25:15.502180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nimport pandas as pd\n\n# Создадим DataFrame с координатами для визуализации и кластерами\npca_df = pd.DataFrame(latent_2d, columns=[\"PC1\", \"PC2\"])\npca_df[\"Leiden Clusters\"] = leiden_clusters\n\n# Визуализация с Plotly для PCA\nfig_pca = px.scatter(pca_df, x=\"PC1\", y=\"PC2\", color=\"Leiden Clusters\", \n                     title=\"Leiden Clustering on Latent Representations (PCA)\", \n                     color_continuous_scale='blackbody', \n                     labels={\"Leiden Clusters\": \"Leiden Clusters\"})\nfig_pca.update_traces(marker=dict(size=10, opacity=0.8, line=dict(width=1, color='DarkSlateGrey')),\n                     selector=dict(mode='markers'))\n\nfig_pca.show()\n\n# Визуализация с Plotly для UMAP\numap_df = pd.DataFrame(umap_latent, columns=[\"UMAP1\", \"UMAP2\"])\numap_df[\"Leiden Clusters\"] = leiden_clusters\n\nfig_umap = px.scatter(umap_df, x=\"UMAP1\", y=\"UMAP2\", color=\"Leiden Clusters\", \n                      title=\"Leiden Clustering with UMAP\", \n                      color_continuous_scale='algae',\n                      labels={\"Leiden Clusters\": \"Leiden Clusters\"})\nfig_umap.update_traces(marker=dict(size=10, opacity=0.8, line=dict(width=1, color='DarkSlateGrey')),\n                      selector=dict(mode='markers'))\n\nfig_umap.show()\n# 'aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance',\n#              'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg',\n#              'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl',\n#              'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric',\n#              'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys',\n#              'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet',\n#              'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges',\n#              'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl',\n#              'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn',\n#              'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu',\n#              'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar',\n#              'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn',\n#              'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid',\n#              'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr',\n#              'ylorrd'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.503051Z","iopub.status.idle":"2025-03-10T16:25:15.503365Z","shell.execute_reply":"2025-03-10T16:25:15.503254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Соотносим выданные кластеры с изначальными тестовыми данными\ntest_data[\"leiden_cluster\"] = leiden_clusters\ntest_data[test_data[\"leiden_cluster\"] == 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.503943Z","iopub.status.idle":"2025-03-10T16:25:15.504228Z","shell.execute_reply":"2025-03-10T16:25:15.504094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered_pheno","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.505162Z","iopub.status.idle":"2025-03-10T16:25:15.505438Z","shell.execute_reply":"2025-03-10T16:25:15.505336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification[\"PRIMARY_HISTOLOGY\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.506058Z","iopub.status.idle":"2025-03-10T16:25:15.506590Z","shell.execute_reply":"2025-03-10T16:25:15.506378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 1. Фильтруем ID из test_data, относящиеся к кластеру 1\n# cluster_1_ids = test_data.loc[test_data[\"leiden_cluster\"] == 0, \"ID\"]\n\n# # 2. Выбираем соответствующие ID в pheno\n# pheno_filtered = pheno[pheno[\"GENOMIC_MUTATION_ID\"].isin(cluster_1_ids)]\n\n# # 3. Соотносим выбранные ID с class\n# class_filtered = classification[classification[\"COSMIC_PHENOTYPE_ID\"].isin(pheno_filtered[\"COSMIC_PHENOTYPE_ID\"])]\n\n# # 4. Выводим статистику по колонке histology\n# histology_counts = class_filtered[\"PRIMARY_HISTOLOGY\"].value_counts()\n\n# # 5. Выводим результат\n# print(histology_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.507306Z","iopub.status.idle":"2025-03-10T16:25:15.507650Z","shell.execute_reply":"2025-03-10T16:25:15.507520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Фильтруем ID для первого кластера\ncluster_1_ids = test_data.loc[test_data[\"leiden_cluster\"] == 39, \"ID\"]\npheno_filtered_1 = pheno[pheno[\"GENOMIC_MUTATION_ID\"].isin(cluster_1_ids)]\nclass_filtered_1 = classification[classification[\"COSMIC_PHENOTYPE_ID\"].isin(pheno_filtered_1[\"COSMIC_PHENOTYPE_ID\"])]\nhistology_counts_1 = class_filtered_1[\"PRIMARY_HISTOLOGY\"].value_counts()\n\n# Фильтруем ID для второго кластера\ncluster_2_ids = test_data.loc[test_data[\"leiden_cluster\"] == 4, \"ID\"]\npheno_filtered_2 = pheno[pheno[\"GENOMIC_MUTATION_ID\"].isin(cluster_2_ids)]\nclass_filtered_2 = classification[classification[\"COSMIC_PHENOTYPE_ID\"].isin(pheno_filtered_2[\"COSMIC_PHENOTYPE_ID\"])]\nhistology_counts_2 = class_filtered_2[\"PRIMARY_HISTOLOGY\"].value_counts()\n\n# Фильтруем ID для второго кластера\ncluster_3_ids = test_data.loc[test_data[\"leiden_cluster\"] == 3, \"ID\"]\npheno_filtered_3 = pheno[pheno[\"GENOMIC_MUTATION_ID\"].isin(cluster_3_ids)]\nclass_filtered_3 = classification[classification[\"COSMIC_PHENOTYPE_ID\"].isin(pheno_filtered_3[\"COSMIC_PHENOTYPE_ID\"])]\nhistology_counts_3 = class_filtered_3[\"PRIMARY_HISTOLOGY\"].value_counts()\n\n# Фильтруем ID для второго кластера\ncluster_4_ids = test_data.loc[test_data[\"leiden_cluster\"] == 6, \"ID\"]\npheno_filtered_4 = pheno[pheno[\"GENOMIC_MUTATION_ID\"].isin(cluster_4_ids)]\nclass_filtered_4 = classification[classification[\"COSMIC_PHENOTYPE_ID\"].isin(pheno_filtered_4[\"COSMIC_PHENOTYPE_ID\"])]\nhistology_counts_4 = class_filtered_4[\"PRIMARY_HISTOLOGY\"].value_counts()\n\n# Объединяем по индексам (PRIMARY_SITE)\nhistology_comparison = pd.concat([histology_counts_1, histology_counts_2, histology_counts_3, histology_counts_4], axis=1, keys=[\"Cluster_1\", \"Cluster_2\", \"Cluster_3\", \"Cluster_4\"])\n\n# Заполняем пропущенные значения нулями\nhistology_comparison = histology_comparison.fillna(0).astype(int)\n\npd.set_option(\"display.expand_frame_repr\", False)  # Отключает перенос строк\npd.set_option(\"display.width\", 200)  # Устанавливает максимальную ширину строки\n\n# Выводим результат\nprint(histology_comparison)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.508326Z","iopub.status.idle":"2025-03-10T16:25:15.508567Z","shell.execute_reply":"2025-03-10T16:25:15.508466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cluster_1 = test_data.loc[test_data[\"leiden_cluster\"] == 39, \"PRIMARY_HISTOLOGY\"].value_counts()\ncluster_2 = test_data.loc[test_data[\"leiden_cluster\"] == 4, \"PRIMARY_HISTOLOGY\"].value_counts()\ncluster_3 = test_data.loc[test_data[\"leiden_cluster\"] == 6, \"PRIMARY_HISTOLOGY\"].value_counts()\ncluster_4 = test_data.loc[test_data[\"leiden_cluster\"] == 3, \"PRIMARY_HISTOLOGY\"].value_counts()\n# Объединяем данные в одну таблицу по индексу PRIMARY_HISTOLOGY\nhistology_comparison = pd.concat([cluster_1, cluster_2, cluster_3, cluster_4], axis=1)\n\n# Переименовываем столбцы для каждого кластера\nhistology_comparison.columns = [\"Cluster_1\", \"Cluster_2\", \"Cluster_3\", \"Cluster_4\"]\n\n# Заполняем пропущенные значения нулями\nhistology_comparison = histology_comparison.fillna(0).astype(int)\n\n# Выводим результат\nprint(histology_comparison)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:25:15.509142Z","iopub.status.idle":"2025-03-10T16:25:15.509424Z","shell.execute_reply":"2025-03-10T16:25:15.509316Z"}},"outputs":[],"execution_count":null}]}